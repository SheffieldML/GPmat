<!--#include virtual="../software/header.shtml" -->

<!--#include virtual="../software/style.html" -->

<head>

<title>Gaussian Processes - MATLAB Software</title>

</head>



<body><div class="section">



<h1>Gaussian Process Software</h1>



<p>This page describes examples of how to use the Gaussian Process

Software (GP).




    <p>The GP software can be downloaded
    <a href="http://www.cs.man.ac.uk/neill-bin/software/downloadForm.cgi?toolbox=gp">here</a>.
    <h2>Release Information</h2>
    <p><b>Current release is 0.137</b>.
    <p>As well as downloading the GP software you need to obtain the toolboxes specified below. <b>These can be downloaded using the <i>same</i> password you get from registering for the  GP software.</b>
    <table>
    <tr>
    <td width="65%"><b>Toolbox</b></td>
    <td width="35%"><b>Version</b></td>
    </tr><tr><td><a href="http://www.cs.man.ac.uk/~neill/netlab/downloadFiles/vrs3p3">NETLAB</a></td><td> 3.3</td></tr>
<tr><td><a href="http://www.cs.man.ac.uk/~neill/mocap/downloadFiles/vrs0p136">MOCAP</a></td><td> 0.136</td></tr>
<tr><td><a href="http://www.cs.man.ac.uk/~neill/ndlutil/downloadFiles/vrs0p162">NDLUTIL</a></td><td> 0.162</td></tr>
<tr><td><a href="http://www.cs.man.ac.uk/~neill/prior/downloadFiles/vrs0p22">PRIOR</a></td><td> 0.22</td></tr>
<tr><td><a href="http://www.cs.man.ac.uk/~neill/mltools/downloadFiles/vrs0p138">MLTOOLS</a></td><td> 0.138</td></tr>
<tr><td><a href="http://www.cs.man.ac.uk/~neill/mocap/downloadFiles/vrs0p136">MOCAP</a></td><td> 0.136</td></tr>
<tr><td><a href="http://www.cs.man.ac.uk/~neill/optimi/downloadFiles/vrs0p132">OPTIMI</a></td><td> 0.132</td></tr>
<tr><td><a href="http://www.cs.man.ac.uk/~neill/datasets/downloadFiles/vrs0p1371">DATASETS</a></td><td> 0.1371</td></tr>
<tr><td><a href="http://www.cs.man.ac.uk/~neill/kern/downloadFiles/vrs0p226">KERN</a></td><td> 0.226</td></tr>
<tr><td><a href="http://www.cs.man.ac.uk/~neill/noise/downloadFiles/vrs0p141">NOISE</a></td><td> 0.141</td></tr>
</table>



Minor updates to gpLoadResult for allowing different functions for loading in data.



<h3>Version 0.136</h4>





Changes to gpReadFromFID for compatibility with C++ code.



<h4>Version 0.135</h4>



Modifications by Carl Henrik Ek for compatability with the SGPLVM toolbox.



<h4>Version 0.134</h4>



<p>Updates to allow deconstruction of model files when writing to disk (gpWriteResult, gpLoadResult, gpDeconstruct, gpReconstruct).



<h4>Version 0.133</h4>



<p>Updates for running a GPLVM/GP using the data's inner product matrix for Interspeech synthesis demos.



<h4>Version 0.132</h4>



<p>Examples transfered from oxford toolbox, variational approximation from Titsias added as an option with 'dtcvar'.



<h4>Version 0.131</h4>



<p>Changes to allow compatibility with SGPLVM and NCCA toolboxes.



<h4>Version 0.13</h4>



<p>Changes to allow more flexibility in optimisation of beta.



<h4>Version 0.12</h4>



<p>Various minor changes for enabling back constraints in hierarchical

GP-LVM models.



<h4>Version 0.11</h4>



<p>Changes include the use of the optimiDefaultConstraint('positive') to

obtain the function to constrain beta to be positive (which now

returns 'exp' rather than 'negLogLogit' which was previously the

default). Similarly default optimiser is now given by a command in

optimiDefaultOptimiser.



<h4>Version 0.1</h4>



<p>The first version which is spun out of the FGPLVM toolbox. The

corresponding FGPLVM toolbox is 0.15.





<p>Release 0.1 splits away the Gaussian process section of the FGPLVM

toolbox into this separate toolbox.



<h2>Other GP related software</h2>



<p>The GP-LVM C++ software is available from <a

href="/~neill/gplvmcpp/">here</a>.



<p>The IVM C++ software is available from <a

href="/~neill/ivmcpp/">here</a>.



<p>The MATLAB IVM toolbox is available here <a

href="/~neill/ivm/">here</a>.



<p>The original MATLAB GP-LVM toolbox is available here <a

href="/~neill/gplvm/">here</a>.





<h2>Examples</h2>





<h3>Functions from Gaussians</h3>



<p>This example shows how points which look like they come from a

function to be sampled from a Gaussian distribution. The sample is 25

dimensional and is from a Gaussian with a particular covariance.



<pre>

&gt;&gt; demGpSample

</pre>



<p><center><img src="gpSample.png" width ="50%"><img

src="gpCovariance.png" width ="50%"><br> <i>Left</i> A single, 25

dimensional, sample from a Gaussian distribution. <i>Right</i> the

covariance matrix of the Gaussian distribution..  </center>





<h3>Joint Distribution over two Variables</h3>



<p>Gaussian processes are about conditioning a Gaussian distribution

on the training data to make the test predictions. To illustrate this

process, we can look at the joint distribution over two variables.



<p>&gt;&gt; demGpCov2D([1 2])



<p>Gives the joint distribution for <i>f</i><sub>1</sub> and

<i>f</i><sub>2</sub>. The plots show the joint distributions as well

as the conditional for <i>f</i><sub>2</sub> given

<i>f</i><sub>1</sub>.



<p><center><img src="demGpCov2D1_2_3.png" width ="50%"><img

src="demGpCov2D1_5_3.png" width ="50%"><br> <i>Left</i> Blue line is

contour of joint distribution over the variables <i>f</i><sub>1</sub>

and <i>f</i><sub>2</sub>. Green line indicates an observation of

<i>f</i><sub>1</sub>. Red line is conditional distribution of

<i>f</i><sub>2</sub> given <i>f</i><sub>1</sub>. <i>Right</i> Similar

for <i>f</i><sub>1</sub> and <i>f</i><sub>5</sub>.  </center>







<h3>Different Samples from Gaussian Processes</h3>



A script is provided which samples from a Gaussian process with the

provided covariance function.



<pre>

&gt;&gt; gpSample('rbf', 10, [1 1], [-3 3], 1e5)

</pre>



<p>will give 10 samples from an RBF covariance function with a

parameter vector given by [1 1] (inverse width 1, variance 1) across

the range -3 to 3 on the <i>x</i>-axis. The random seed will be set to

1e5.



<pre>

&gt;&gt; gpSample('rbf', 10, [16 1], [-3 3], 1e5)

</pre>



<p>is similar, but the inverse width is now set to 16 (length scale 0.25).



<p><center><img

src="gpSampleRbfSamples10Seed100000InverseWidth1Variance1.png" width

="50%"><img

src="gpSampleRbfSamples10Seed100000InverseWidth16Variance1.png" width

="50%"><br> <i>Left</i> samples from an RBF style covariance function

with length scale 1. <i>Right</i> samples from an RBF style covariance

function with length scale 0.25.  </center>



<p>Other covariance functions can be sampled, an interesting one is

the MLP covariance which is non stationary and can produce point

symmetric functions,





<pre>

&gt;&gt; gpSample('mlp', 10, [100 100 1], [-1 1], 1e5)

</pre>



gives 10 samples from the MLP covariance function where the &quot;bias

variance&quot; is 100 (basis functions are centered around the origin

with standard deviation of 10) and the &quot;weight variance&quot; is

100.



<pre>

&gt;&gt; gpSample('mlp', 10, [100 1e-16 1], [-1 1], 1e5)

</pre>



gives 10 samples from the MLP covariance function where the &quot;bias

variance&quot; is approximately zero (basis functions are placed on

the origin) and the &quot;weight variance&quot; is 100.



<p><center><img

src="gpSampleMlpSamples10Seed100000WeightVariance100BiasVariance100Variance1.png"

width ="50%"><img

src="gpSampleMlpSamples10Seed100000WeightVariance100BiasVariance1e-16Variance1.png"

width ="50%"><br> <i>Left</i> samples from an MLP style covariance

function with bias and weight variances set to 100. <i>Right</i>

samples from an MLP style covariance function with weight variance 100

and bias variance approximately zero.  </center>





<h3>Posterior Samples</h3>



<p>Gaussian processes are non-parametric models. They are specified by their covariance function and a mean function. When combined with data observations a posterior Gaussian process is induced. The demos below show samples from that posterior.



<pre>

&gt;&gt;  gpPosteriorSample('rbf', 5, [1 1], [-3 3], 1e5)

</pre>



and 



<pre>

&gt;&gt;  gpPosteriorSample('rbf', 5, [16 1], [-3 3], 1e5)

</pre>



<p><center><img

src="gpPosteriorSampleRbfSamples5Seed100000InverseWidth1Variance1bw.png" width

="50%"><img

src="gpPosteriorSampleRbfSamples5Seed100000InverseWidth16Variance1bw.png" width

="50%"><br> <i>Left</i> samples from the posterior induced by an RBF style covariance function

with length scale 1 and 5 &quot;training&quot; data points taken from a sine wave. <i>Right</i> Similar but for a length scale of 0.25.  </center>











<h3>Simple Interpolation Demo</h3>





<p>This simple demonstration plots, consecutively, an increasing

number of data points, followed by an interpolated fit through the

data points using a Gaussian process. This is a noiseless system, and

the data is sampled from a GP with a known covariance function. The

curve is then recovered with minimal uncertainty after only nine data

points are included. The code is run with



<pre>

&gt;&gt; demInterpolation

</pre>



<p><center><img src="demInterpolation3.png" width ="50%"><img

src="demInterpolation4.png" width ="50%"><br>



Gaussian process prediction <i>left</i> after two points with a new

data point sampled <i>right</i> after the new data point is included

in the prediction.<br> 



<img src="demInterpolation7.png" width

="50%"><img src="demInterpolation8.png" width ="50%"><br>



Gaussian process prediction <i>left</i> after five points with a four

new data point sampled <i>right</i> after all nine data points are

included.<br> </center>



<h3>Simple Regression Demo</h3>



<p>The regression demo very much follows the format of the

interpolation demo. Here the difference is that the data is sampled

with noise. Fitting a model with noise means that the regression will

not necessarily pass right through each data point.



The code is run with



<pre>

&gt;&gt; demRegression

</pre>





<p><center><img src="demRegression3.png" width ="50%"><img

src="demRegression4.png" width ="50%"><br>



Gaussian process prediction <i>left</i> after two points with a new

data point sampled <i>right</i> after the new data point is included

in the prediction.<br> <img src="demRegression7.png" width ="50%"><img

src="demRegression8.png" width ="50%"><br>



Gaussian process prediction <i>left</i> after five points with a four

new data point sampled <i>right</i> after all nine data points are

included.<br> </center>



<h3>Optimizing Hyper Parameters</h3>



<p>One of the advantages of Gaussian processes over pure kernel

interpretations of regression is the ability to select the hyper

parameters of the kernel automatically. The demo



<pre>

&gt;&gt; demOptimiseGp

</pre>



<p>shows a series of plots of a Gaussian process with different length

scales fitted to six data points. For each plot there is a

corresponding plot of the log likelihood. The log likelihood peaks for

a length scale equal to 1. This was the length scale used to generate

the data.



<p><center><img src="demOptimiseGp1.png" width ="33%"><img

src="demOptimiseGp3.png" width ="33%"><img src="demOptimiseGp5.png"

width ="33%"><br><img src="demOptimiseGp7.png" width ="33%"><img

src="demOptimiseGp9.png" width ="33%"><img src="demOptimiseGp11.png"

width ="33%"><br><img src="demOptimiseGp13.png" width ="33%"><img

src="demOptimiseGp15.png" width ="33%"><img src="demOptimiseGp17.png"

width ="33%"><br>From top left to bottom right, Gaussian process

regression applied to the data with an increasing length scale. The

length scales used were 0.05, 0.1, 0.25, 0.5, 1, 2, 4, 8 and

16.<br><img src="demOptimiseGp18.png" width ="50%"><br>Log-log plot of

the log likelihood of the data against the length scales. The log

likelihood is shown as a solid line. The log likelihood is made up of

a data fit term (the quadratic form) shown by a dashed line and a

complexity term (the log determinant) shown by a dotted line. The data

fit is larger for short length scales, the complexity is larger for

long length scales. The combination leads to a maximum around the true

length scale value of 1.</center>



<h3>Regression over Motion Capture Markers</h3>



<p>As a simple example of regression for real data we consider a motion capture data set. The data is <a href="http://accad.osu.edu/research/mocap/mocap_data.htm">from Ohio State University</a>. In the example script we perform Gaussian process regression with time as the input and the x,y,z position of the marker attached to the left ankle. To demonstrate the behavior of the model when the marker is lost, we remove data from This code can be run with



<pre> &gt;&gt; demStickGp1 </pre> 



<p>The code will optimize hyper parameters and show plots of the posterior process through the training data and the missing test points.



<p>The result of the script is given in the plot below.  



<p><center><img src="demStickGp1Out1.png" width

="30%"> <img src="demStickGp1Out2.png" width

="30%"> <img src="demStickGp1Out3.png" width

="30%"><br> Gaussian process regression through the x (left), y (middle) and z (right) position of the left ankle. Training data is shown as black spots, test points removed to simulate a lost marker are shown as circles, posterior mean

prediction is shown as a black line and two standard deviations are

given as grey shading.</center>



<p>Notice how the error bars are tight except in the region where the training data is missing and in the region where the training data disappears.



<h3>Sparse Pseudo-input Gaussian Processes</h3>



<p>The sparse approximation used in this toolbox is based on the

Sparse Pseudo-input Gaussian Process model described by <a

href="/neill-bin/publications/bibpage.cgi?keyName=Snelson:pseudo05&printAbstract=1">Snelson

and Ghahramani</a>. Also provided are the extensions suggested by <a

href="/neill-bin/publications/bibpage.cgi?keyName=Quinonero:unifying05">Qui&ntilde;onero-Candela

and Rasmussen</a>. They provide a unifying terminology for describing

these approximations which we shall use in what follows.



<p>There are three demos provided for Gaussian process regression in

1-D. They each use a different form of likelihood approximation. The

first demonstration uses the &quot;projected latent variable&quot;

approach first described by <a

href="/neill-bin/publications/bibpage.cgi?keyName=Csato:sparse02&printAbstract=1">Csato

and Opper</a> and later used by <a

href="/neill-bin/publications/bibpage.cgi?keyName=Seeger:fast03&printAbstract=1">Seeger

<i>et al.</i></a>. In the terminology of Qui&ntilde;onero-Candela and

Rasmussen (QR-terminology) this is known as the &quot;deterministic

training conditional&quot; (DTC) approximation.



<p>To use this approximation the following script can be run.



<pre> &gt;&gt; demSpgp1dGp1 </pre> 



<p>The result of the script is

given in the plot below.  



<p><center><img src="demSpgp1dGp1.png" width

="50%"><br> Gaussian process using the DTC approximation with nine

inducing variables. Data is shown as black spots, posterior mean

prediction is shown as a black line and two standard deviations are

given as grey shading.</center>



<p>The improved approximation suggested by Snelson and Ghahramani, in

QR-terminology this is known as the fully independent training

conditional (FITC). To try this approximation run the following script



<pre> &gt;&gt; demSpgp1dGp2 </pre>



<p>The result of the script is given on the left of the plot below.



<p><center><img src="demSpgp1dGp2.png" width="49%"><img

src="demSpgp1dGp3.png" width="49%"><br>



<i>Left</i>: Gaussian process using the FITC approximation with nine

inducing variables. Data is shown as black spots, posterior mean

prediction is shown as a black line and two standard deviations are

given as grey shading. <i>Right</i>: Similar but for the PITC

approximation, again with nine inducing variables.</center>



<p>At the <a href="http://www.dcs.shef.ac.uk/ml/gprt/">Sheffield

Gaussian Process Round Table</a> Lehel Csato pointed out that the

Bayesian Committee Machine of <a

href="/neill-bin/publications/bibpage.cgi?group=bcm&printAbstract=1">Schwaighofer

and Tresp</a> can also be viewed within the same framework. This idea

is formalised in <a

href="/neill-bin/publications/bibpage.cgi?keyName=Quinonero:unifying05&printAbstract=1">Qui&ntilde;onero-Candela

and Rasmussen's</a> review. This approximation is known as the

&quot;partially independent training conditional&quot; (PITC) in

QR-terminology. To try this approximation run the following script



<pre>

&gt;&gt; demSpgp1dGp3

</pre>



<p>The result of the script is given on the right of the plot above.



<p>Finally we can compare these results to the result from the full

Gaussian process on the data with the correct hyper-parameters. To do

this the following script can be run.



<pre>

&gt;&gt; demSpgp1dGp4

</pre>



<p>The result of the script is given in the plot below.



<p><center><img src="demSpgp1dGp4.png" width="50%"><br> Full Gaussian

process on the toy data with the correct hyper-parameters. Data is

shown as black spots, posterior mean prediction is shown as a black

line and two standard deviations are given as grey shaded

area.</center>



</div>

</body>

<body><p><center>Page updated on Fri Jul 22 16:22:15 2011</center></body><!--#include virtual="../software/footer.shtml" -->

